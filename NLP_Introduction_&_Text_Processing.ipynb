{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Computational Linguistics and how does it relate to NLP?\n"
      ],
      "metadata": {
        "id": "YUfCgr7Mnhgh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zVzm3c1naa3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Computational Linguistics is the field where we try to understand how human language works and teach computers to process it in a meaningful way.\n",
        "It focuses on building models and rules that explain how sentences are formed, how meaning is created, and how language is structured.\n",
        "Natural Language Processing (NLP) is the practical use of these ideas. It applies those models to build real-life tools like chatbots,\n",
        "translators, voice assistants, and sentiment analysis systems.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Briefly describe the historical evolution of Natural Language Processing.\n"
      ],
      "metadata": {
        "id": "5kR5cAjPn5b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Natural Language Processing (NLP) has evolved step by step with advances in computing and AI. In the 1950s–60s,\n",
        "it started with rule-based systems, where language was processed using hand-written grammar rules. In the 1980s–90s,\n",
        "the focus shifted to statistical and machine learning methods, using large datasets to learn language patterns.\n",
        "From the 2010s onwards, deep learning and neural networks transformed NLP, leading to highly accurate systems like translators, chatbots, and voice assistants.\n",
        "'''"
      ],
      "metadata": {
        "id": "QIhp-Zvbn6Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: List and explain three major use cases of NLP in today’s tech industry.\n"
      ],
      "metadata": {
        "id": "LpmOHSvHoF6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Three use cases are :\n",
        "1. Language Translation : NLP enables automatic translation between languages, like Google Translate, helping people communicate globally.\n",
        "2. Chatbots: NLP helps machines understand user queries and respond like humans, as seen in customer support bots, Alexa, and Siri.\n",
        "3. Sentiment Analysis: Companies use NLP to analyze customer reviews, social media posts, and feedback to understand public opinion and improve services.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "-rLg-uwQoHDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is text normalization and why is it essential in text processing tasks?\n"
      ],
      "metadata": {
        "id": "E9M1s6rDopxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Text normalization is the process of cleaning and standardizing text so that it becomes easier for computers to understand and process.\n",
        "It includes steps like converting text to lowercase, removing punctuation, correcting spelling, expanding contractions, and handling special characters.\n",
        "It is essential because real-world text is often messy and inconsistent. Normalization helps reduce noise, improves accuracy,\n",
        "and ensures better performance of NLP models in tasks like sentiment analysis, search, and text classification.\n",
        "'''"
      ],
      "metadata": {
        "id": "rp1Ma9ohoqmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare and contrast stemming and lemmatization with suitable\n",
        "examples."
      ],
      "metadata": {
        "id": "P1qgeawdss7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Stemming and lemmatization are both text processing techniques used in NLP to reduce words to their base form, but they work in different ways.\n",
        "Stemming is a simpler and faster approach that removes prefixes or suffixes from words without considering grammar or meaning. Because of this,\n",
        "the resulting word may not always be a real or meaningful word, for example, “running” becomes “run” and “studies” becomes “studi”.\n",
        "lemmatization is more advanced and meaningful because it considers the context and grammatical structure of the word. It reduces words to\n",
        "their actual dictionary form, known as the lemma, such as “running” becoming “run” and “better” becoming “good”.\n",
        "'''"
      ],
      "metadata": {
        "id": "6OGO7LtYst4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”"
      ],
      "metadata": {
        "id": "Tnk0I8HntPKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text)\n",
        "\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL3s-fTntQOC",
        "outputId": "dffef1f9-a998-49d1-d351-c640cf8b34c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@xyz.com', 'hr@xyz.com', 'john.doe@xyz.org', 'jenny_clarke126@mail.co.us', 'partners@xyz.biz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”"
      ],
      "metadata": {
        "id": "RbOAmw74tZfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download tokenizer (only first time)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added this line as suggested by the error message\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"\\nWord Frequency:\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYbFFtI2tUXN",
        "outputId": "ee7886c3-e288-4733-bb31-51f1c4b13637"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Word Frequency:\n",
            "[(',', 7), ('.', 4), ('NLP', 3), ('and', 3), ('is', 2), ('of', 2), ('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given text.\n"
      ],
      "metadata": {
        "id": "p3E7PJiuttJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def proper_noun_annotator(text):\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"PROPN\":\n",
        "            print(f\"{token.text} → Proper Noun\")\n",
        "\n",
        "text = \"John works at Google in New York and studies NLP at Stanford University.\"\n",
        "proper_noun_annotator(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6RV2K-ttcUy",
        "outputId": "15fd7f32-42d0-4248-9199-3b2153d65d8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John → Proper Noun\n",
            "Google → Proper Noun\n",
            "New → Proper Noun\n",
            "York → Proper Noun\n",
            "NLP → Proper Noun\n",
            "Stanford → Proper Noun\n",
            "University → Proper Noun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim."
      ],
      "metadata": {
        "id": "0LtN7I-IuZUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Dataset\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Step 1: Tokenization and preprocessing\n",
        "tokenized_data = [word_tokenize(sentence.lower()) for sentence in dataset]\n",
        "\n",
        "# Step 2: Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 3: Test the model\n",
        "print(model.wv[\"language\"])\n",
        "print(model.wv.most_similar(\"word\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0utF88KluSIc",
        "outputId": "430c8cad-2b20-41d7-89f0-16475f51fd3d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "[-9.5806085e-03  8.9441882e-03  4.1653137e-03  9.2335343e-03\n",
            "  6.6461298e-03  2.9213182e-03  9.8062111e-03 -4.4229976e-03\n",
            " -6.7968965e-03  4.2171725e-03  3.7335777e-03 -5.6669810e-03\n",
            "  9.6989106e-03 -3.5659580e-03  9.5487935e-03  8.3945523e-04\n",
            " -6.3411104e-03 -1.9765138e-03 -7.3686293e-03 -2.9793803e-03\n",
            "  1.0386854e-03  9.4879130e-03  9.3503986e-03 -6.6033388e-03\n",
            "  3.4822454e-03  2.2797708e-03 -2.4912679e-03 -9.2314119e-03\n",
            "  1.0282570e-03 -8.1718396e-03  6.3123670e-03 -5.7999776e-03\n",
            "  5.5352398e-03  9.8330248e-03 -1.6240444e-04  4.5257602e-03\n",
            " -1.8113367e-03  7.3653422e-03  3.9460384e-03 -9.0134088e-03\n",
            " -2.3964681e-03  3.6255287e-03 -1.0223665e-04 -1.2030570e-03\n",
            " -1.0514213e-03 -1.6768404e-03  6.0620415e-04  4.1634240e-03\n",
            " -4.2499253e-03 -3.8340162e-03 -5.2803134e-05  2.6898083e-04\n",
            " -1.6909892e-04 -4.7887340e-03  4.3159435e-03 -2.1736636e-03\n",
            "  2.1060717e-03  6.6301227e-04  5.9748082e-03 -6.8420274e-03\n",
            " -6.8199760e-03 -4.4755712e-03  9.4431778e-03 -1.5884656e-03\n",
            " -9.4326762e-03 -5.3903682e-04 -4.4508227e-03  6.0071596e-03\n",
            " -9.5821600e-03  2.8594565e-03 -9.2493063e-03  1.2525463e-03\n",
            "  6.0056061e-03  7.4005253e-03 -7.6154172e-03 -6.0545066e-03\n",
            " -6.8385820e-03 -7.9242960e-03 -9.4967606e-03 -2.1271543e-03\n",
            " -8.3613099e-04 -7.2621531e-03  6.7897881e-03  1.1199901e-03\n",
            "  5.8384286e-03  1.4718205e-03  7.9351984e-04 -7.3687923e-03\n",
            " -2.1785437e-03  4.3192604e-03 -5.0813761e-03  1.1349940e-03\n",
            "  2.8907768e-03 -1.5272817e-03  9.9381926e-03  8.3554955e-03\n",
            "  2.4154300e-03  7.1279611e-03  5.8829025e-03 -5.5807778e-03]\n",
            "[('tokenization', 0.21880948543548584), ('modeling', 0.21611614525318146), ('embedding', 0.19551844894886017), ('enables', 0.16922107338905334), ('word2vec', 0.15158729255199432), ('words', 0.14219118654727936), ('meaning', 0.10859505832195282), ('technique', 0.09939780831336975), ('allows', 0.09636874496936798), ('clean', 0.09305424243211746)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews."
      ],
      "metadata": {
        "id": "vpm-rV87unOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As a data scientist in a fintech startup, I would start by collecting and cleaning the customer reviews, removing noise like special characters,\n",
        "URLs, emojis, duplicate entries, and converting text to lowercase.\n",
        "Next, I would perform text preprocessing such as tokenization, stop-word removal, lemmatization, and handling spelling variations\n",
        "to make the data consistent and machine-readable.\n",
        "After that, I would apply feature extraction techniques like TF-IDF or word embeddings to convert text into numerical form.\n",
        "Then, I would use NLP models for tasks such as sentiment analysis to understand customer opinions, topic modeling to identify common issues or requests,\n",
        "and keyword extraction to spot trends.\n",
        "Finally, I would visualize and interpret the results to provide actionable insights to product, support, and business teams,\n",
        "helping them improve customer experience and decision-making.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "y99TwPOpudUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}